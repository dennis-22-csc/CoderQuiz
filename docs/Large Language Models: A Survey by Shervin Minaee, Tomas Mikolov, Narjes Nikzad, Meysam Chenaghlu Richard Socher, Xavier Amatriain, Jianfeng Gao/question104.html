<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   Formatted Document
  </title>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript">
  </script>
  <style>
  </style>
 </head>
 <body>
  <div class="question" id="question104">
   <h2>
    104,Which process reduces data sparsity in early neural language models?,Embedding words into vectors,Using n-grams,Applying smoothing,Data augmentation,
   </h2>
   <p>
    The correct answer is:
   </p>
   <p>
    ✅
    <strong>
     Embedding words into vectors
    </strong>
   </p>
   <h3>
    <strong>
     Explanation:
    </strong>
   </h3>
   <p>
    Early neural language models
    <strong>
     suffered from data sparsity
    </strong>
    because they relied on discrete word representations, where each word had a unique ID, leading to a massive vocabulary with many unseen words.
   </p>
   <p>
    <strong>
     Word embeddings
    </strong>
    (such as Word2Vec, GloVe, or FastText) helped address this by:
   </p>
   <p>
    *
    <strong>
     Mapping words into dense, low-dimensional vector spaces
    </strong>
    instead of one-hot encoding.
   </p>
   <p>
    *
    <strong>
     Capturing semantic relationships
    </strong>
    between words (e.g., "king" and "queen" have similar vector structures).
   </p>
   <p>
    *
    <strong>
     Reducing sparsity
    </strong>
    , allowing models to generalize better even with limited data.
   </p>
   <h4>
    <strong>
     Why not the other options?
    </strong>
   </h4>
   <p>
    ❌
    <strong>
     Using n-grams
    </strong>
    – N-grams suffer from sparsity issues, especially for large vocabularies.
   </p>
   <p>
    ❌
    <strong>
     Applying smoothing
    </strong>
    – Smoothing helps in
    <strong>
     probabilistic language models
    </strong>
    but is not the primary solution for neural models.
   </p>
   <p>
    ❌
    <strong>
     Data augmentation
    </strong>
    – While it can help in some cases, it is not the main method for reducing data sparsity in early neural models.
   </p>
   <h3>
    <strong>
     Final Answer:
    </strong>
   </h3>
   <p>
    ✅
    <strong>
     Embedding words into vectors
    </strong>
   </p>
  </div>
 </body>
</html>