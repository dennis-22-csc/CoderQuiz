<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   Formatted Document
  </title>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript">
  </script>
  <style>
  </style>
 </head>
 <body>
  <div class="question" id="question130">
   <h2>
    130,What allows embeddings to compute semantic similarity?,Their dimensionality,Their training method,Their hidden space,Their task-specific tuning,
   </h2>
   <p>
    âœ…
    <strong>
     Their training method
    </strong>
   </p>
   <h3>
    <strong>
     Explanation:
    </strong>
   </h3>
   <p>
    Embeddings compute semantic similarity based on how they are
    <strong>
     trained
    </strong>
    . Specifically, embeddings are trained to map words or phrases into continuous vector spaces where semantically similar words are represented by vectors that are closer together. The
    <strong>
     training method
    </strong>
    is crucial in determining the relationships between words in this vector space. Common techniques like
    <strong>
     Word2Vec
    </strong>
    ,
    <strong>
     GloVe
    </strong>
    , or
    <strong>
     transformer-based models
    </strong>
    (e.g., BERT) learn these embeddings from large text corpora, capturing semantic meaning by minimizing a loss function that encourages semantically related words to have similar representations.
   </p>
   <h4>
    <strong>
     Why not the others?
    </strong>
   </h4>
   <p>
    *
    <strong>
     Dimensionality
    </strong>
    : While dimensionality affects the representational power, it's the
    <strong>
     training method
    </strong>
    that determines how well embeddings capture semantic similarity.
   </p>
   <p>
    *
    <strong>
     Hidden space
    </strong>
    : The hidden space refers to the internal representation in the model but doesn't directly compute similarity in the way training methods do.
   </p>
   <p>
    *
    <strong>
     Task-specific tuning
    </strong>
    : While tuning can refine embeddings for a specific task, it is the
    <strong>
     training method
    </strong>
    that generally determines how semantic similarity is captured in embeddings.
   </p>
   <p>
    Thus,
    <strong>
     their training method
    </strong>
    is the correct answer.
   </p>
  </div>
 </body>
</html>