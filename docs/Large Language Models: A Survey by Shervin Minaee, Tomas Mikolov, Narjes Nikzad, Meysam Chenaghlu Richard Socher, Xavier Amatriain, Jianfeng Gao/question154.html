<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   Formatted Document
  </title>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript">
  </script>
  <style>
  </style>
 </head>
 <body>
  <div class="question" id="question154">
   <h2>
    154,Which model introduced the concept of masked language modeling?,GPT,BERT,ELECTRA,RoBERTa,
   </h2>
   <p>
    âœ…
    <strong>
     BERT
    </strong>
   </p>
   <h3>
    <strong>
     Explanation:
    </strong>
   </h3>
   <p>
    BERT (Bidirectional Encoder Representations from Transformers) introduced the concept of
    <strong>
     Masked Language Modeling (MLM)
    </strong>
    , where certain tokens in the input text are masked, and the model is trained to predict those masked tokens. This bidirectional approach allows BERT to capture context from both sides of the masked word.
   </p>
  </div>
 </body>
</html>