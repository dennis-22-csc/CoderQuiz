<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   Formatted Document
  </title>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript">
  </script>
  <style>
  </style>
 </head>
 <body>
  <div class="question" id="question198">
   <h2>
    198,What is the key innovation in RoBERTa compared to BERT?,New language tasks,Modified hyperparameters and training strategies,Masked sequence pre-training,Bidirectional training,Modified hyperparameters and training strategies,Pre-trained LLMs
   </h2>
   <p>
    ✅
    <strong>
     Modified hyperparameters and training strategies
    </strong>
   </p>
   <h3>
    <strong>
     Explanation:
    </strong>
   </h3>
   <p>
    RoBERTa (
    <strong>
     Robustly Optimized BERT Approach
    </strong>
    ) is an improved version of BERT that
    <strong>
     modifies its training strategies and hyperparameters
    </strong>
    to enhance performance.
   </p>
   <h3>
    <strong>
     Key Innovations in RoBERTa:
    </strong>
   </h3>
   <p>
    1.
    <strong>
     Longer Training with More Data
    </strong>
   </p>
   <p>
    * RoBERTa is trained on
    <strong>
     more data
    </strong>
    and for
    <strong>
     longer epochs
    </strong>
    compared to BERT.
   </p>
   <p>
    * It removes
    <strong>
     Next Sentence Prediction (NSP)
    </strong>
    , which was part of BERT's training, leading to better performance.
   </p>
   <p>
    2.
    <strong>
     Larger Batch Sizes &amp; Learning Rates
    </strong>
   </p>
   <p>
    * RoBERTa uses
    <strong>
     bigger batch sizes
    </strong>
    and
    <strong>
     optimized learning rates
    </strong>
    , making training more effective.
   </p>
   <p>
    3.
    <strong>
     Dynamic Masking for MLM (Masked Language Modeling)
    </strong>
   </p>
   <p>
    * Unlike BERT, which
    <strong>
     masks tokens once and reuses the same masked dataset
    </strong>
    , RoBERTa
    <strong>
     dynamically remasks tokens
    </strong>
    for better generalization.
   </p>
   <p>
    4.
    <strong>
     More Pretraining Data
    </strong>
   </p>
   <p>
    * It is trained on datasets
    <strong>
     10x larger
    </strong>
    than BERT (160GB of text instead of 16GB), improving its ability to understand language.
   </p>
   <h3>
    <strong>
     Why Other Options Are Incorrect:
    </strong>
   </h3>
   <p>
    *
    <strong>
     New language tasks
    </strong>
    → RoBERTa
    <strong>
     does not introduce new tasks
    </strong>
    ; it only modifies how existing tasks are trained.
   </p>
   <p>
    *
    <strong>
     Masked sequence pre-training
    </strong>
    → Both
    <strong>
     BERT and RoBERTa use MLM
    </strong>
    , so this is
    <strong>
     not unique
    </strong>
    to RoBERTa.
   </p>
   <p>
    *
    <strong>
     Bidirectional training
    </strong>
    → BERT
    <strong>
     already
    </strong>
    uses bidirectional attention, so this is
    <strong>
     not a new feature
    </strong>
    in RoBERTa.
   </p>
   <p>
    *
    <strong>
     Pre-trained LLMs
    </strong>
    → This applies to
    <strong>
     many models
    </strong>
    , not specifically RoBERTa.
   </p>
   <p>
    Thus, the correct answer is
    <strong>
     Modified hyperparameters and training strategies
    </strong>
    .
   </p>
  </div>
 </body>
</html>