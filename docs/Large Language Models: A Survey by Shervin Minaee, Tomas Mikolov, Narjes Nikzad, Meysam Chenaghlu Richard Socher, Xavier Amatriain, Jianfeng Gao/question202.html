<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   Formatted Document
  </title>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript">
  </script>
  <style>
  </style>
 </head>
 <body>
  <div class="question" id="question202">
   <h2>
    202,What is the pre-training objective of ELECTRA?,Predict masked tokens,Replaced token detection,Next token prediction,Sequence-to-sequence modeling,
   </h2>
   <p>
    ✅
    <strong>
     Replaced Token Detection
    </strong>
   </p>
   <h3>
    <strong>
     Explanation:
    </strong>
   </h3>
   <p>
    ELECTRA (
    <strong>
     Efficiently Learning an Encoder that Classifies Token Replacements Accurately
    </strong>
    ) introduces
    <strong>
     Replaced Token Detection (RTD)
    </strong>
    as its
    <strong>
     pre-training objective
    </strong>
    instead of
    <strong>
     Masked Language Modeling (MLM)
    </strong>
    used in BERT.
   </p>
   <h3>
    <strong>
     How Replaced Token Detection Works?
    </strong>
   </h3>
   <p>
    1. Instead of masking tokens like BERT,
    <strong>
     ELECTRA replaces some tokens with plausible alternatives
    </strong>
    generated by a small generator model.
   </p>
   <p>
    2. The main model (
    <strong>
     discriminator
    </strong>
    ) is trained to
    <strong>
     detect whether each token was replaced or not
    </strong>
    .
   </p>
   <h3>
    <strong>
     Why RTD Instead of MLM?
    </strong>
   </h3>
   <p>
    *
    <strong>
     More efficient training
    </strong>
    → RTD forces the model to learn from
    <strong>
     all input tokens
    </strong>
    instead of only masked tokens (MLM learns from \~15% of tokens).
   </p>
   <p>
    *
    <strong>
     Improves performance
    </strong>
    → ELECTRA achieves better accuracy than BERT with
    <strong>
     less computational cost
    </strong>
    .
   </p>
   <h3>
    <strong>
     Why Other Options Are Incorrect?
    </strong>
   </h3>
   <p>
    *
    <strong>
     Predict masked tokens
    </strong>
    → Used in
    <strong>
     BERT, RoBERTa
    </strong>
    (not ELECTRA).
   </p>
   <p>
    *
    <strong>
     Next token prediction
    </strong>
    → Used in
    <strong>
     GPT models
    </strong>
    (ELECTRA does not predict next tokens).
   </p>
   <p>
    *
    <strong>
     Sequence-to-sequence modeling
    </strong>
    → Used in
    <strong>
     T5, BART
    </strong>
    for text generation (ELECTRA is not sequence-to-sequence).
   </p>
   <p>
    Thus, the correct answer is
    <strong>
     Replaced Token Detection
    </strong>
    .
   </p>
  </div>
 </body>
</html>