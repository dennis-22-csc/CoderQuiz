<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   Formatted Document
  </title>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript">
  </script>
  <style>
  </style>
 </head>
 <body>
  <div class="question" id="question175">
   <h2>
    175,What is a major limitation of RNN-based models compared to Transformers?,Inability to learn long-term dependencies,Lack of bidirectional processing,Limited training dataset compatibility,Sequential computation bottlenecks,
   </h2>
   <p>
    âœ…
    <strong>
     Sequential computation bottlenecks
    </strong>
   </p>
   <h3>
    <strong>
     Explanation:
    </strong>
   </h3>
   <p>
    A major limitation of RNN-based models compared to Transformers is the
    <strong>
     sequential computation bottleneck
    </strong>
    . RNNs process data one step at a time, making them slower and less efficient for training, especially for long sequences. Transformers, on the other hand, leverage parallel computation through their attention mechanism, enabling faster training and better handling of long-term dependencies.
   </p>
  </div>
 </body>
</html>