<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   Formatted Document
  </title>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript">
  </script>
  <style>
  </style>
 </head>
 <body>
  <div class="question" id="question142">
   <h2>
    142,Attention is key for parallel processing in transformers.,True,False,
   </h2>
   <p>
    âœ…
    <strong>
     True
    </strong>
   </p>
   <h3>
    <strong>
     Explanation:
    </strong>
   </h3>
   <p>
    The attention mechanism in transformers is key for parallel processing because it allows the model to focus on different parts of the input sequence simultaneously, rather than sequentially like in RNNs. This parallelism helps speed up training and inference, making transformers more efficient than traditional models that process data step-by-step.
   </p>
  </div>
 </body>
</html>