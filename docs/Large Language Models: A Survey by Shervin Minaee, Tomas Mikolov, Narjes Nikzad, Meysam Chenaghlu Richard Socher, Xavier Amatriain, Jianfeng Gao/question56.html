<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   Formatted Document
  </title>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript">
  </script>
  <style>
  </style>
 </head>
 <body>
  <div class="question" id="question556">
   <h2>
    56,Which type of LLM architecture is primarily used for language understanding tasks?,Decoder-only,Encoder-only,Encoder-decoder,Both Decoder-only and Encoder-only,
   </h2>
   <p>
    The correct answer is:
   </p>
   <p>
    <strong>
     Encoder-only
    </strong>
   </p>
   <h3>
    Explanation:
   </h3>
   <p>
    Encoder-only architectures, such as
    <strong>
     BERT (Bidirectional Encoder Representations from Transformers)
    </strong>
    , are primarily used for
    <strong>
     language understanding tasks
    </strong>
    like:
   </p>
   <p>
    *
    <strong>
     Text classification
    </strong>
   </p>
   <p>
    *
    <strong>
     Named entity recognition (NER)
    </strong>
   </p>
   <p>
    *
    <strong>
     Sentiment analysis
    </strong>
   </p>
   <p>
    *
    <strong>
     Question answering
    </strong>
    (extractive)
   </p>
   <h3>
    Why Encoder-only?
   </h3>
   <p>
    ✅
    <strong>
     Bidirectional Context:
    </strong>
    Encoder models process input text
    <strong>
     in both directions (left and right context)
    </strong>
    , making them well-suited for understanding meaning and relationships between words.
   </p>
   <p>
    ✅
    <strong>
     Strong Representations:
    </strong>
    They generate
    <strong>
     rich contextual embeddings
    </strong>
    for downstream tasks.
   </p>
   <p>
    ✅
    <strong>
     Pretraining on MLM (Masked Language Modeling):
    </strong>
    This helps the model learn deep
    <strong>
     semantic representations
    </strong>
    .
   </p>
   <h3>
    Why Not Other Architectures?
   </h3>
   <p>
    *
    <strong>
     Decoder-only (e.g., GPT-3, LLaMA):
    </strong>
    Optimized for
    <strong>
     language generation
    </strong>
    , not understanding.
   </p>
   <p>
    *
    <strong>
     Encoder-Decoder (e.g., T5, BART):
    </strong>
    Best for
    <strong>
     seq-to-seq tasks
    </strong>
    like translation and summarization, not pure understanding.
   </p>
   <p>
    *
    <strong>
     Both Encoder-only and Decoder-only:
    </strong>
    Not entirely correct because
    <strong>
     decoder-only models focus on generation, not understanding.
    </strong>
   </p>
   <h3>
    Final Answer:
    <strong>
     Encoder-only models
    </strong>
    are best suited for
    <strong>
     language understanding
    </strong>
    tasks.
   </h3>
  </div>
 </body>
</html>